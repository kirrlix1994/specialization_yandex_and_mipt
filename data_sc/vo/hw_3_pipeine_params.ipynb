{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нужно подобрать оптимальные параметры pipeline с линейными моделями и векторизацией текстов.\n",
    "Код для проверки качества представлен в скрипте text_classification_params_checker.py, \n",
    "а пример набора параметров в text_classification_params_example.json. \n",
    "Чекер с вашими параметрами должен отработать за 1 минуту на машинке для проверки.\n",
    "Для сравнения на text_classification_params_example.json чекер работает 15 секунд."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "import heapq\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_categories = fetch_20newsgroups().target_names\n",
    "all_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмём темы из одного раздела, возможно, их будет сложнее отличать друг от друга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categories = [\n",
    "    'sci.electronics',\n",
    "    'sci.space',\n",
    "    'sci.med'\n",
    "]\n",
    "train_data = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
    "test_data = fetch_20newsgroups(subset='test', categories=categories, remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для векторизации текстов воспользуемся CountVectorizer, он представляет документ как мешок слов. Можно всячески варировать извлечение признаков (убирать редкие слова, убирать частые слова, убирать слова общей лексики, брать биграмы и т.д.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(min_df=5, ngram_range=(1, 2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(train_data.data[23], \"\\n\\n\\n\\n\")\n",
    "# print(train_data.data[943], \"\\n\\n\\n\\n\")\n",
    "# print(train_data.data[1232], \"\\n\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1778x10885 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 216486 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_feature_matrix = count_vectorizer.fit_transform(train_data.data)\n",
    "sparse_feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1778, 10885)\n"
     ]
    }
   ],
   "source": [
    "# To many features !\n",
    "print(sparse_feature_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  for k, v in count_vectorizer.vocabulary_.items():\n",
    "#     print(k)\n",
    "#     print(v, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_2_words = {\n",
    "    v: k\n",
    "    for k, v in count_vectorizer.vocabulary_.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим логистическую регрессию для предсказания темы документа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo = LogisticRegression()\n",
    "algo.fit(sparse_feature_matrix, train_data.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Слова с наибольшим положительным весом, являются характерными словами темы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "circuit,  electronics,  power,  chips,  parts,  them,  the number,  used,  tv,  ve\n",
      "msg,  medical,  my,  blood,  disease,  doctor,  health,  treatment,  your,  needles\n",
      "space,  orbit,  nasa,  thanks for,  launch,  earth,  sorry,  moon,  spacecraft,  solar\n"
     ]
    }
   ],
   "source": [
    "W = algo.coef_.shape[1]\n",
    "for c in algo.classes_:\n",
    "    topic_words = [\n",
    "        num_2_words[w_num]\n",
    "        for w_num in heapq.nlargest(10, range(W), key=lambda w: algo.coef_[c, w])\n",
    "    ]\n",
    "    print(',  '.join(topic_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним качество на фолдах с качеством на трейне и на отложенном тесте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.8487395   0.84550562  0.83426966  0.83943662  0.82768362]\n",
      "0.839127002447\n"
     ]
    }
   ],
   "source": [
    "algo = LogisticRegression()\n",
    "arr = cross_val_score(algo, sparse_feature_matrix, train_data.target, cv=5, scoring='accuracy')\n",
    "print(arr)\n",
    "print(np.mean(arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Почему это неправильная кроссвалидация?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo.fit(sparse_feature_matrix, train_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98031496062992129"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(algo.predict(sparse_feature_matrix), train_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79289940828402372"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(algo.predict(count_vectorizer.transform(test_data.data)), test_data.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Мы видим переобучение, это проклятие размерности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.8487395   0.84550562  0.83426966  0.83943662  0.82768362]\n",
      "0.839127002447\n"
     ]
    }
   ],
   "source": [
    "algo = LogisticRegression(penalty='l2', C=1)\n",
    "arr = cross_val_score(algo, sparse_feature_matrix, train_data.target, cv=5, scoring='accuracy')\n",
    "print(arr)\n",
    "print(np.mean(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo.fit(sparse_feature_matrix, train_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98031496062992129"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(algo.predict(sparse_feature_matrix), train_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79289940828402372"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(algo.predict(count_vectorizer.transform(test_data.data)), test_data.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавление регуляризатора уменьшает отличие на трейне и тесте, но ухудшает качество. Поиграйтесь дома с параметрами регуляризации, чтобы получить максимальное качество."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы не делать векторизацию и обучение раздельно, есть удобный класс Pipeline. Он позволяет объединить в цепочку последовательность действий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    (\"vectorizer\", CountVectorizer(min_df=5, ngram_range=(1, 2))), \n",
    "    (\"algo\", LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=5,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
       "       ...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(train_data.data, train_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98031496062992129"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(pipeline.predict(train_data.data), train_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79289940828402372"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(pipeline.predict(test_data.data), test_data.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Значения примерно такие же как мы получали ранее, делаяя шаги раздельно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При кроссвалидации нужно, чтобы CountVectorizer не обучался на тесте (иначе объекты становятся зависимыми). Pipeline позволяет это просто сделать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.83753501  0.84550562  0.82303371  0.83943662  0.83050847]\n",
      "0.835203886829\n"
     ]
    }
   ],
   "source": [
    "pipeline = make_pipeline(CountVectorizer(min_df=5, ngram_range=(1, 2)), LogisticRegression())\n",
    "arr = cross_val_score(pipeline, train_data.data, train_data.target, cv=5, scoring='accuracy')\n",
    "print(arr)\n",
    "print(np.mean(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.80269815  0.81618887  0.80067568]\n",
      "0.806520896951\n"
     ]
    }
   ],
   "source": [
    "pipeline = make_pipeline(CountVectorizer(min_df=5, ngram_range=(1, 2)), LogisticRegression())\n",
    "arr = cross_val_score(pipeline, train_data.data, train_data.target, cv=3, scoring='accuracy')\n",
    "print(arr)\n",
    "print(np.mean(arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В Pipeline можно добавлять новые шаги препроцессинга данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warning-и в данном случае это нормально, не пугайтесь. Это будет исправлено в следующих версиях библиотеки sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.90725126  0.88701518  0.87162162]\n",
      "0.888629354481\n"
     ]
    }
   ],
   "source": [
    "pipeline = make_pipeline(CountVectorizer(min_df=1, ngram_range=(1, 1)), \n",
    "                         TfidfTransformer(norm='l2', use_idf=True, smooth_idf=False, \n",
    "                                         sublinear_tf=True ), \n",
    "                         LogisticRegression(penalty='l2', C=100))\n",
    "arr = cross_val_score(pipeline, train_data.data, train_data.target, cv=3, scoring='accuracy')\n",
    "print(arr)\n",
    "print(np.mean(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('countvectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "  ...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(train_data.data, train_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98200224971878514"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(pipeline.predict(train_data.data), train_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85122569737954357"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(pipeline.predict(test_data.data), test_data.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество стало немного лучше"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание\n",
    "\n",
    "1. Поиграйтесь с параметрами регуляризации, параметрами CountVectorizer и TfidfTransformer, чтобы получить максимальное качество. (нужно будет отправить на проверку, checker будет выложет позже)\n",
    "2. Постройте список важных слов и словосочетаний для каждой темы (на основе значений коэффициентов). Это чисто по фану"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params_example = json.load(open(\"hw_3_params_example.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'count_vectorizer_params': {'min_df': 5, 'ngram_range': [1, 2]},\n",
       " 'logistic_regression_params': {'C': 1},\n",
       " 'tfidf_transformer_params': {'norm': 'l1'}}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_params = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vowpal Wabbit on GitHub: https://github.com/JohnLangford/vowpal_wabbit\n",
    "\n",
    "Vowpal Wabbit Tutorial: https://github.com/JohnLangford/vowpal_wabbit/wiki/Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'vowpalwabbit'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-9cf9c03ced3e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mvowpalwabbit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msklearn_vw\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mVWClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'vowpalwabbit'"
     ]
    }
   ],
   "source": [
    "from vowpalwabbit.sklearn_vw import VWClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'make_pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-2d7a434c9448>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpipeline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_pipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mngram_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTfidfTransformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVWClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'make_pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "pipeline = make_pipeline(CountVectorizer(min_df=5, ngram_range=(1, 2)), TfidfTransformer(), VWClassifier())\n",
    "arr = cross_val_score(pipeline, train_data.data, train_data.target, cv=5, scoring='accuracy')\n",
    "print(arr)\n",
    "print(np.mean(arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "не работает :( VWClassifier только для бинарной классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "with open('train', 'w') as f:\n",
    "    for text, target in zip(train_data.data, train_data.target):\n",
    "        f.write('{} | {}\\n'.format(target + 1, ' '.join(re.findall('\\w+', text.lower()))))\n",
    "        \n",
    "with open('test', 'w') as f:\n",
    "    for text, target in zip(test_data.data, test_data.target):\n",
    "        f.write('{} | {}\\n'.format(target + 1, ' '.join(re.findall('\\w+', text.lower()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_regressor = vw.model\n",
      "Num weight bits = 18\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "creating cache_file = train.cache\n",
      "Reading datafile = train\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "1.000000 1.000000            1            1.0        3        1       43\n",
      "1.000000 1.000000            2            2.0        1        3       90\n",
      "0.750000 0.500000            4            4.0        2        2      348\n",
      "0.750000 0.750000            8            8.0        2        3       89\n",
      "0.687500 0.625000           16           16.0        1        2       57\n",
      "0.687500 0.687500           32           32.0        3        3       30\n",
      "0.562500 0.437500           64           64.0        3        1       22\n",
      "0.460938 0.359375          128          128.0        1        1       33\n",
      "0.367188 0.273438          256          256.0        1        1      116\n",
      "0.300781 0.234375          512          512.0        2        2      103\n",
      "0.236328 0.171875         1024         1024.0        3        3      200\n",
      "0.168142 0.168142         2048         2048.0        1        1      184 h\n",
      "0.145695 0.123348         4096         4096.0        2        2      230 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 1601\n",
      "passes used = 5\n",
      "weighted example sum = 8005.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.118644 h\n",
      "total feature number = 1516440\n"
     ]
    }
   ],
   "source": [
    "!rm train.cache\n",
    "!vw -d train  -c --passes 10 -f vw.model --oaa 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "only testing\r\n",
      "predictions = test.out\r\n",
      "Num weight bits = 18\r\n",
      "learning rate = 0.5\r\n",
      "initial_t = 0\r\n",
      "power_t = 0.5\r\n",
      "using no cache\r\n",
      "Reading datafile = test\r\n",
      "num sources = 1\r\n",
      "average  since         example        example  current  current  current\r\n",
      "loss     last          counter         weight    label  predict features\r\n",
      "0.000000 0.000000            1            1.0        1        1      127\r\n",
      "0.000000 0.000000            2            2.0        3        3       43\r\n",
      "0.000000 0.000000            4            4.0        2        2      295\r\n",
      "0.250000 0.500000            8            8.0        2        3        1\r\n",
      "0.250000 0.250000           16           16.0        2        2      179\r\n",
      "0.281250 0.312500           32           32.0        2        3       52\r\n",
      "0.265625 0.250000           64           64.0        3        1      214\r\n",
      "0.179688 0.093750          128          128.0        2        2       69\r\n",
      "0.175781 0.171875          256          256.0        1        3      105\r\n",
      "0.189453 0.203125          512          512.0        1        1      222\r\n",
      "0.172852 0.156250         1024         1024.0        3        3      173\r\n",
      "\r\n",
      "finished run\r\n",
      "number of examples per pass = 1183\r\n",
      "passes used = 1\r\n",
      "weighted example sum = 1183.000000\r\n",
      "weighted label sum = 0.000000\r\n",
      "average loss = 0.177515\r\n",
      "total feature number = 197080\r\n"
     ]
    }
   ],
   "source": [
    "!vw -i vw.model -t test -p test.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8224852071005917"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "hits = 0\n",
    "with open('test', 'r') as f_features, open('test.out', 'r') as f_predictions:\n",
    "    for line_features, line_predictions in zip(f_features, f_predictions):\n",
    "        count += 1\n",
    "        hits += int(line_features.split()[0]) == int(line_predictions)\n",
    "        \n",
    "1. * hits / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7844463229078613"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!rm train.cache\n",
    "!vw -d train  -c --passes 10 -f vw.model --ect 3 --quiet\n",
    "!vw -i vw.model -t test -p test.out --quiet\n",
    "\n",
    "count = 0\n",
    "hits = 0\n",
    "with open('test', 'r') as f_features, open('test.out', 'r') as f_predictions:\n",
    "    for line_features, line_predictions in zip(f_features, f_predictions):\n",
    "        count += 1\n",
    "        hits += int(line_features.split()[0]) == int(line_predictions)\n",
    "        \n",
    "1. * hits / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!rm train.cache\n",
    "!vw -d train  -c --passes 10 -f vw.model --csoaa 3 --quiet\n",
    "!vw -i vw.model -t test -p test.out --quiet\n",
    "\n",
    "count = 0\n",
    "hits = 0\n",
    "with open('test', 'r') as f_features, open('test.out', 'r') as f_predictions:\n",
    "    for line_features, line_predictions in zip(f_features, f_predictions):\n",
    "        count += 1\n",
    "        hits += int(line_features.split()[0]) == int(line_predictions)\n",
    "        \n",
    "1. * hits / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
